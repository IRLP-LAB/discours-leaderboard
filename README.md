# ğŸ¯ Discourse Resolution Evaluation System

<div align="center">

![Version](https://img.shields.io/badge/version-1.0.0-blue.svg)
![Python](https://img.shields.io/badge/python-3.8+-green.svg)
![FastAPI](https://img.shields.io/badge/FastAPI-0.68+-teal.svg)
![License](https://img.shields.io/badge/license-GPL--2.0-red.svg)

**A comprehensive web-based platform for evaluating discourse resolution (coreference) systems using industry-standard metrics across multiple languages.**

[Features](#-features) â€¢ [Installation](#-installation) â€¢ [Usage](#-usage-guide) â€¢ [Documentation](#-api-documentation) â€¢ [Support](#-support)

</div>

---

## ğŸ“‹ Table of Contents

- [Overview](#-overview)
- [Features](#-features)
- [System Architecture](#-system-architecture)
- [Installation](#-installation)
- [Configuration](#%EF%B8%8F-configuration)
- [Usage Guide](#-usage-guide)
- [Evaluation Metrics](#-evaluation-metrics)
- [Project Structure](#-project-structure)
- [API Documentation](#-api-documentation)
- [Troubleshooting](#-troubleshooting)
- [Contributing](#-contributing)
- [License](#-license)
- [Acknowledgments](#-acknowledgments)

---

## ğŸ¯ Overview

The **Discourse Resolution Evaluation System** is a FastAPI-based web application designed for researchers and practitioners working on coreference resolution. It provides automated evaluation of system outputs against gold standard datasets using multiple standardized metrics.

### ğŸ’¡ What is Coreference Resolution?

Coreference resolution is the task of identifying when different expressions in text refer to the same real-world entity. 

**Example:**
```
"John went to the store. He bought milk."
```
The system needs to recognize that **"He"** refers to **"John"**.

### ğŸŒŸ Why This System?

| Feature | Description |
|---------|-------------|
| âœ… **Standardized Evaluation** | Uses industry-standard metrics (MUC, B-CUBED, CEAF, BLANC) |
| ğŸŒ **Multi-language Support** | Evaluate systems across different languages |
| âš¡ **Real-time Results** | Get instant feedback on your system's performance |
| ğŸ† **Leaderboard** | Compare your results with other participants |
| ğŸ¨ **Easy to Use** | Simple web interface for uploading and evaluating files |

---

## âœ¨ Features

### ğŸ‘¥ For Participants

<table>
<tr>
<td width="50%">

#### ğŸ“¤ File Upload & Evaluation
- Upload system output files (.txt format)
- Instant scoring using Perl-based CorScorer
- Support for multiple languages

</td>
<td width="50%">

#### ğŸ“Š Comprehensive Metrics
- **MUC** - Link-based evaluation
- **B-CUBED** - Mention-based evaluation
- **CEAF-M/E** - Entity alignment
- **BLANC** - Bilateral assessment

</td>
</tr>
<tr>
<td width="50%">

#### ğŸ“ˆ Evaluation History
- Track all submissions
- View historical scores
- Download past results

</td>
<td width="50%">

#### ğŸ† Leaderboard
- Public rankings per language
- Compare with other systems
- Track your progress

</td>
</tr>
</table>

### ğŸ” For Administrators

<table>
<tr>
<td>

- **ğŸ‘¤ User Management** - Create and manage participant accounts
- **ğŸŒ Language Configuration** - Add, edit, or remove supported languages
- **ğŸ“ Gold Dataset Management** - Upload and manage reference datasets
- **ğŸ“Š System Monitoring** - View statistics and activity logs
- **ğŸ¯ Leaderboard Control** - Manage public rankings and scores

</td>
</tr>
</table>

### ğŸŒ Public Features

- ğŸ  **Homepage Leaderboard** - View top-performing systems
- ğŸ“Š **Statistics Dashboard** - Overall participation metrics
- ğŸ“± **Responsive Design** - Works on desktop and mobile

---

## ğŸ—ï¸ System Architecture
```mermaid
graph TB
    A[Web Browser] --> B[FastAPI Application]
    B --> C[Authentication System]
    B --> D[File Management]
    B --> E[Scoring Engine]
    C --> F[(MySQL Database)]
    D --> F
    E --> G[Perl CorScorer]
    D --> H[File System Storage]
    
    style A fill:#e1f5ff
    style B fill:#fff3e0
    style F fill:#f3e5f5
    style G fill:#e8f5e9
    style H fill:#fce4ec
```

<details>
<summary><b>ğŸ“ Architecture Details (Click to expand)</b></summary>
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Web Browser                           â”‚
â”‚              (HTML/CSS/JavaScript Interface)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FastAPI Application                       â”‚
â”‚                    (Python Backend)                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ Auth System  â”‚  â”‚   File       â”‚  â”‚   Scoring    â”‚     â”‚
â”‚  â”‚  (bcrypt)    â”‚  â”‚  Management  â”‚  â”‚   Engine     â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚              â”‚
                     â–¼              â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  MySQL Database    â”‚  â”‚  Perl CorScorer  â”‚
        â”‚  (User data,       â”‚  â”‚  (Evaluation)    â”‚
        â”‚   Scores, etc.)    â”‚  â”‚                  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚      File System Storage            â”‚
        â”‚  â€¢ uploads/         (user files)    â”‚
        â”‚  â€¢ gold_datasets/   (reference)     â”‚
        â”‚  â€¢ scorer/          (Perl scripts)  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

</details>

---

## ğŸš€ Installation

### ğŸ“‹ Prerequisites

Before installing, ensure you have the following:

| Software | Version | Purpose | Installation Link |
|----------|---------|---------|-------------------|
| ğŸ **Python** | 3.8+ | Backend runtime | [python.org](https://www.python.org/downloads/) |
| ğŸ—„ï¸ **MySQL** | 8.0+ | Database | [mysql.com](https://dev.mysql.com/downloads/) |
| ğŸ“œ **Perl** | 5.x | Scoring engine | [Windows](https://strawberryperl.com/) / [Linux/Mac](https://www.perl.org/get.html) |

#### ğŸ”§ Required Perl Modules
```bash
cpan install Algorithm::Munkres
cpan install Math::Combinatorics
```

---

### ğŸ“¥ Step-by-Step Installation

#### **Step 1: Clone or Download the Repository**
```bash
# Option A: Clone with git
git clone <repository-url>
cd discourse-evaluation-system

# Option B: Download and extract ZIP file
# Then navigate to the extracted folder
```

#### **Step 2: Set Up Python Virtual Environment**
```bash
# Create virtual environment
python -m venv venv

# Activate virtual environment
# Windows:
venv\Scripts\activate

# Linux/Mac:
source venv/bin/activate
```

#### **Step 3: Install Python Dependencies**
```bash
pip install fastapi uvicorn python-multipart jinja2 mysql-connector-python bcrypt
```

<details>
<summary><b>ğŸ“¦ Alternative: Using requirements.txt (Click to expand)</b></summary>

Create a `requirements.txt` file:
```txt
fastapi>=0.68.0
uvicorn>=0.15.0
python-multipart>=0.0.5
jinja2>=3.0.0
mysql-connector-python>=8.0.0
bcrypt>=3.2.0
```

Then install:
```bash
pip install -r requirements.txt
```

</details>

#### **Step 4: Set Up MySQL Database**

<details>
<summary><b>ğŸ—„ï¸ Database Setup SQL (Click to expand)</b></summary>
```sql
-- Open MySQL command line or MySQL Workbench
-- Create database
CREATE DATABASE coref_eval_system;

-- Create user (change credentials as needed)
CREATE USER 'harsh'@'localhost' IDENTIFIED BY 'harsh';
GRANT ALL PRIVILEGES ON coref_eval_system.* TO 'harsh'@'localhost';
FLUSH PRIVILEGES;

-- Use the database
USE coref_eval_system;

-- Create users table
CREATE TABLE users (
    id INT AUTO_INCREMENT PRIMARY KEY,
    username VARCHAR(100) UNIQUE NOT NULL,
    email VARCHAR(255) UNIQUE NOT NULL,
    password_hash VARCHAR(255) NOT NULL,
    is_active BOOLEAN DEFAULT TRUE,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Create languages table
CREATE TABLE languages (
    id INT AUTO_INCREMENT PRIMARY KEY,
    language_code VARCHAR(10) UNIQUE NOT NULL,
    language_name VARCHAR(100) NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Create gold_datasets table
CREATE TABLE gold_datasets (
    id INT AUTO_INCREMENT PRIMARY KEY,
    language_id INT NOT NULL,
    filename VARCHAR(255) NOT NULL,
    file_path VARCHAR(500) NOT NULL,
    uploaded_by VARCHAR(100),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (language_id) REFERENCES languages(id) ON DELETE CASCADE
);

-- Create user_evaluations table
CREATE TABLE user_evaluations (
    id INT AUTO_INCREMENT PRIMARY KEY,
    user_id INT NOT NULL,
    language_id INT NOT NULL,
    uploaded_filename VARCHAR(255) NOT NULL,
    file_path VARCHAR(500) NOT NULL,
    muc_recall DECIMAL(10,4),
    muc_precision DECIMAL(10,4),
    muc_f1 DECIMAL(10,4),
    bcub_recall DECIMAL(10,4),
    bcub_precision DECIMAL(10,4),
    bcub_f1 DECIMAL(10,4),
    ceafm_recall DECIMAL(10,4),
    ceafm_precision DECIMAL(10,4),
    ceafm_f1 DECIMAL(10,4),
    ceafe_recall DECIMAL(10,4),
    ceafe_precision DECIMAL(10,4),
    ceafe_f1 DECIMAL(10,4),
    blanc_recall DECIMAL(10,4),
    blanc_precision DECIMAL(10,4),
    blanc_f1 DECIMAL(10,4),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (user_id) REFERENCES users(id),
    FOREIGN KEY (language_id) REFERENCES languages(id)
);
```

</details>

#### **Step 5: Configure Database Connection**

Edit the `DB_CONFIG` section in `main.py`:
```python
DB_CONFIG = {
    'host': 'localhost',
    'database': 'coref_eval_system',
    'user': 'harsh',        # ğŸ‘ˆ Change to your MySQL username
    'password': 'harsh'     # ğŸ‘ˆ Change to your MySQL password
}
```

#### **Step 6: Set Up Scorer Files**

1. Create the `scorer` directory in your project root
2. Download CorScorer from the official source
3. Place these files in `scorer/` directory:
   - `scorer.pl` - Main scoring script
   - `CorScorer.pm` - Perl module for scoring
   - Any other required Perl modules

> **ğŸ“ Note**: The CorScorer package should be obtained from the official CoNLL shared task or SemEval repositories.

#### **Step 7: Verify Installation**
```bash
# Test database connection
python -c "import mysql.connector; print('âœ… MySQL OK')"

# Test Perl
perl -v

# Test Perl modules
perl -e "use Algorithm::Munkres; print 'âœ… Munkres OK\n';"
perl -e "use Math::Combinatorics; print 'âœ… Combinatorics OK\n';"
```

---

## âš™ï¸ Configuration

### ğŸ” Database Configuration

Edit `main.py` to match your MySQL setup:
```python
DB_CONFIG = {
    'host': 'localhost',      # Database host
    'database': 'coref_eval_system',  # Database name
    'user': 'harsh',          # Your MySQL username
    'password': 'harsh'       # Your MySQL password
}
```

### ğŸŒ Environment Variables (Production)

For production deployment, use environment variables:
```python
import os

DB_CONFIG = {
    'host': os.getenv('DB_HOST', 'localhost'),
    'database': os.getenv('DB_NAME', 'coref_eval_system'),
    'user': os.getenv('DB_USER', 'harsh'),
    'password': os.getenv('DB_PASSWORD', 'harsh')
}
```

### ğŸ­ Demo Mode

The system includes a demo mode that works without a database:

| User Type | Username | Password |
|-----------|----------|----------|
| ğŸ‘¨â€ğŸ’¼ Admin | `admin` | `admin123` |
| ğŸ‘¤ Regular User | `testuser` | `user123` |

**Demo Languages:**
- ğŸ‡®ğŸ‡³ Hindi (hi)
- ğŸ‡¬ğŸ‡§ English (en)

> Demo mode automatically activates if database connection fails.

---

## ğŸ“– Usage Guide

### ğŸš€ Starting the Server
```bash
# 1. Activate virtual environment
# Windows:
venv\Scripts\activate
# Linux/Mac:
source venv/bin/activate

# 2. Start the server
python main.py

# 3. Access the application
# Open browser and navigate to: http://localhost:8000
```

**Expected Output:**
```
Starting Coreference Evaluation System...
Demo credentials:
  Admin: admin/admin123
  User: testuser/user123
Access at: http://localhost:8000

System checks:
  âœ… Perl available: True
  âœ… Database connection: OK
```

---

### ğŸ‘¥ For Participants

#### ğŸ” **Step 1: Login**

1. Navigate to `http://localhost:8000/login`
2. Enter your credentials (or use demo credentials)
3. Click **"Sign In"**

<kbd>![Login Screen](https://via.placeholder.com/600x300/667eea/ffffff?text=Login+Screen)</kbd>

#### ğŸ“¤ **Step 2: Upload Files for Evaluation**

1. **Select Language** from the dropdown menu
2. **Choose File** - Click and select your system output file (.txt format)
3. **Evaluate** - Click "Evaluate File" button
4. **Wait** - Processing typically takes 10-60 seconds
5. **View Results** - Scores appear in the results panel

<kbd>![Upload Interface](https://via.placeholder.com/600x300/764ba2/ffffff?text=Upload+Interface)</kbd>

#### ğŸ“Š **Step 3: View History**

- Your evaluation history appears on the right side
- Shows: Date, filename, language, and all metric scores
- Automatically updates after each evaluation

<kbd>![Evaluation History](https://via.placeholder.com/600x300/2a5298/ffffff?text=Evaluation+History)</kbd>

#### ğŸ† **Step 4: View Leaderboard**

- Visit the homepage to see public leaderboards
- Rankings are per language
- Shows your best score for each language

---

### ğŸ” For Administrators

#### ğŸ›ï¸ **Access Admin Panel**

1. Login with admin credentials
2. Automatically redirected to `/admin`

#### ğŸŒ **Add New Languages**

1. Go to **"Language Management"** tab
2. Enter **language code** (e.g., "fr" for French)
3. Enter **full language name** (e.g., "French")
4. Click **"Add Language"**

<kbd>![Language Management](https://via.placeholder.com/600x200/1e3c72/ffffff?text=Language+Management)</kbd>

#### ğŸ“ **Upload Gold Datasets**

1. Go to **"Gold Datasets"** tab
2. Select language from dropdown
3. Choose your reference file (.txt format)
4. Click **"Upload Dataset"**

> âš ï¸ **Important**: Each language must have a gold dataset before participants can evaluate files.

#### ğŸ‘¤ **Manage Users**

1. Go to **"User Management"** tab
2. Fill in username, email, and password
3. Click **"Add User"**
4. Users can then login with their credentials

---

### ğŸ“„ File Format Requirements

#### ğŸ“ System Output Files (.txt)

Your system output must follow the CoNLL/SemEval coreference format:
```
#begin document (doc_id);
doc_id    0    0    John    (1)
doc_id    0    1    went    -
doc_id    0    2    to    -
doc_id    0    3    the    (2
doc_id    0    4    store    2)
doc_id    0    5    .    -
doc_id    1    0    He    (1)
doc_id    1    1    bought    -
doc_id    1    2    milk    (3)
doc_id    1    3    .    -
#end document
```

**Format Details:**

| Column | Description | Example |
|--------|-------------|---------|
| 1 | Document ID | `doc_id` |
| 2 | Part number | `0`, `1`, etc. |
| 3 | Token number | `0`, `1`, `2`, etc. |
| 4 | Word/Token | `John`, `went`, etc. |
| 5 | Coreference | `(1)`, `(2`, `2)`, `-` |

**Coreference Annotations:**
- `(1)` - Singleton mention in chain 1
- `(2` - Mention starts in chain 2
- `2)` - Mention ends in chain 2
- `-` - No coreference annotation

#### ğŸ“š Gold Dataset Files (.txt)

Gold datasets follow the same format as system output files. They represent the correct/reference coreference annotations.

---

## ğŸ“Š Evaluation Metrics

The system provides **five** standard coreference evaluation metrics:

### 1ï¸âƒ£ MUC (Message Understanding Conference)

<table>
<tr>
<td width="30%"><b>ğŸ“Œ What it measures</b></td>
<td>Focuses on the number of links correctly identified between mentions in the same coreference chain</td>
</tr>
<tr>
<td><b>ğŸ§® Formula</b></td>
<td>Based on the minimum number of links needed to form clusters</td>
</tr>
<tr>
<td><b>âœ… Best for</b></td>
<td>Systems that aim to connect mentions accurately</td>
</tr>
</table>

### 2ï¸âƒ£ B-CUBED (BÂ³)

<table>
<tr>
<td width="30%"><b>ğŸ“Œ What it measures</b></td>
<td>Evaluates each mention individually and rewards correct placement in coreference chains</td>
</tr>
<tr>
<td><b>ğŸ§® Formula</b></td>
<td>Averages precision and recall over all mentions</td>
</tr>
<tr>
<td><b>âœ… Best for</b></td>
<td>Getting fine-grained per-mention accuracy</td>
</tr>
</table>

### 3ï¸âƒ£ CEAF-M (Mention-based)

<table>
<tr>
<td width="30%"><b>ğŸ“Œ What it measures</b></td>
<td>Optimal alignment between system and gold mention sets</td>
</tr>
<tr>
<td><b>ğŸ§® Formula</b></td>
<td>Uses Kuhn-Munkres algorithm for optimal matching</td>
</tr>
<tr>
<td><b>âœ… Best for</b></td>
<td>Balanced evaluation of mention identification and clustering</td>
</tr>
</table>

### 4ï¸âƒ£ CEAF-E (Entity-based)

<table>
<tr>
<td width="30%"><b>ğŸ“Œ What it measures</b></td>
<td>Similar to CEAF-M but focuses on entity-level similarity</td>
</tr>
<tr>
<td><b>ğŸ§® Formula</b></td>
<td>Aligns entities (clusters) rather than individual mentions</td>
</tr>
<tr>
<td><b>âœ… Best for</b></td>
<td>Systems focused on entity-level coreference</td>
</tr>
</table>

### 5ï¸âƒ£ BLANC

<table>
<tr>
<td width="30%"><b>ğŸ“Œ What it measures</b></td>
<td>Evaluates both coreference and non-coreference links</td>
</tr>
<tr>
<td><b>ğŸ§® Formula</b></td>
<td>F-score of coreference links combined with F-score of non-coreference links</td>
</tr>
<tr>
<td><b>âœ… Best for</b></td>
<td>Comprehensive evaluation including correct separation of non-coreferent mentions</td>
</tr>
</table>

---

### ğŸ“ˆ Understanding the Scores

Each metric provides **three values**:

| Metric | Description | Range |
|--------|-------------|-------|
| ğŸ“Š **Recall** | Percentage of correct links/mentions found | 0.0 - 1.0 |
| ğŸ¯ **Precision** | Percentage of predicted links/mentions that are correct | 0.0 - 1.0 |
| â­ **F1 Score** | Harmonic mean of recall and precision | 0.0 - 1.0 |

#### Score Interpretation:
```
ğŸŸ¢ 1.0 = Perfect     â–º All predictions are correct
ğŸŸ¡ 0.5 = Moderate    â–º Half of the predictions are correct
ğŸ”´ 0.0 = Poor        â–º No correct predictions
```

---

## ğŸ“ Project Structure
```
discourse-evaluation-system/
â”‚
â”œâ”€â”€ ğŸ“„ main.py                          # Main FastAPI application
â”‚   â”œâ”€â”€ Database configuration
â”‚   â”œâ”€â”€ Authentication & sessions
â”‚   â”œâ”€â”€ File upload handling
â”‚   â”œâ”€â”€ Perl scorer integration
â”‚   â”œâ”€â”€ Route handlers
â”‚   â””â”€â”€ Demo mode functionality
â”‚
â”œâ”€â”€ ğŸ“ templates/                       # HTML templates (Jinja2)
â”‚   â”œâ”€â”€ ğŸ  homepage.html               # Public homepage with leaderboards
â”‚   â”œâ”€â”€ ğŸ” login.html                  # Login page
â”‚   â”œâ”€â”€ ğŸ‘¤ client_dashboard.html      # Participant dashboard
â”‚   â””â”€â”€ ğŸ”§ admin_dashboard.html       # Admin control panel
â”‚
â”œâ”€â”€ ğŸ“ uploads/                        # User-uploaded system output files
â”‚   â””â”€â”€ [user_id]_[timestamp]_[filename].txt
â”‚
â”œâ”€â”€ ğŸ“ gold_datasets/                  # Reference datasets
â”‚   â”œâ”€â”€ ğŸ“‚ lang_1/                    # Language-specific folders
â”‚   â”‚   â””â”€â”€ [timestamp]_[filename].txt
â”‚   â”œâ”€â”€ ğŸ“‚ lang_2/
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ ğŸ“ scorer/                         # Perl scoring scripts
â”‚   â”œâ”€â”€ ğŸ“œ scorer.pl                  # Main scoring script
â”‚   â”œâ”€â”€ ğŸ“œ CorScorer.pm              # Perl module
â”‚   â””â”€â”€ [other Perl dependencies]
â”‚
â”œâ”€â”€ ğŸ“ venv/                          # Python virtual environment
â”‚   â”œâ”€â”€ Scripts/  (Windows)
â”‚   â”œâ”€â”€ bin/      (Linux/Mac)
â”‚   â””â”€â”€ Lib/
â”‚
â”œâ”€â”€ ğŸ“„ requirements.txt               # Python dependencies
â”‚
â””â”€â”€ ğŸ“˜ README.md                      # This file
```

### ğŸ“Œ Key Files Explained

| File/Folder | Purpose |
|-------------|---------|
| `main.py` | Core application with all backend logic, database operations, and API endpoints |
| `templates/` | Frontend HTML pages styled with embedded CSS and JavaScript |
| `uploads/` | Temporary storage for participant submissions |
| `gold_datasets/` | Organized by language, contains reference datasets for evaluation |
| `scorer/` | Contains Perl-based CorScorer package for metric calculation |

---

## ğŸ”Œ API Documentation

### ğŸ” Authentication Endpoints

#### `POST /login`

Login to the system.

**Parameters:**
```json
{
  "username": "string (required)",
  "password": "string (required)"
}
```

**Response:**
- Success: Redirect to `/client` or `/admin`
- Error: Login page with error message

---

#### `GET /logout`

Logout and clear session.

**Response:** Redirect to `/`

---

### ğŸ‘¤ Client Endpoints

#### `GET /client`

Access participant dashboard.

**Authentication:** Required

**Response:** HTML dashboard with evaluation form and history

---

#### `POST /evaluate`

Submit file for evaluation.

**Authentication:** Required

**Parameters:**
```json
{
  "language_id": "integer (required)",
  "file": "file upload (required, .txt only)"
}
```

**Response:**
```json
{
  "success": true,
  "scores": {
    "muc": {
      "recall": 0.85,
      "precision": 0.82,
      "f1": 0.835
    },
    "bcub": {
      "recall": 0.78,
      "precision": 0.80,
      "f1": 0.79
    },
    "ceafm": {
      "recall": 0.75,
      "precision": 0.77,
      "f1": 0.76
    },
    "blanc": {
      "recall": 0.88,
      "precision": 0.85,
      "f1": 0.865
    }
  },
  "message": "Evaluation completed successfully"
}
```

---

### ğŸ”§ Admin Endpoints

#### `GET /admin`

Access admin dashboard.

**Authentication:** Admin only

**Response:** HTML admin panel

---

#### `POST /admin/add_language`

Add new language.

**Authentication:** Admin only

**Parameters:**
```json
{
  "language_code": "string (required, max 10 chars)",
  "language_name": "string (required)"
}
```

**Response:** Redirect to `/admin`

---

#### `POST /admin/upload_gold_dataset`

Upload gold dataset for a language.

**Authentication:** Admin only

**Parameters:**
```json
{
  "language_id": "integer (required)",
  "file": "file upload (required, .txt only)"
}
```

**Response:** Redirect to `/admin`

---

#### `POST /admin/add_user`

Create new participant account.

**Authentication:** Admin only

**Parameters:**
```json
{
  "username": "string (required)",
  "email": "string (required)",
  "password": "string (required)"
}
```

**Response:** Redirect to `/admin`

---

### ğŸŒ Public Endpoints

#### `GET /`

Homepage with statistics and leaderboards.

**Response:** HTML homepage with:
- Language statistics
- Participant count
- Evaluation count
- Per-language leaderboards

---

## ğŸ”§ Troubleshooting

### ğŸš¨ Common Issues and Solutions

<details>
<summary><b>âŒ Issue 1: "Perl not installed" Error</b></summary>

**Symptoms:** Error message when trying to evaluate files

**Solution:**
```bash
# Windows: Install Strawberry Perl
# Download from https://strawberryperl.com/

# Linux:
sudo apt-get install perl

# Mac:
brew install perl

# Verify installation
perl -v
```

</details>

<details>
<summary><b>âŒ Issue 2: "Missing Perl modules" Error</b></summary>

**Symptoms:** Error about `Math::Combinatorics` or `Algorithm::Munkres`

**Solution:**
```bash
# Install via CPAN
cpan install Algorithm::Munkres
cpan install Math::Combinatorics

# Or via cpanm (recommended)
cpanm Algorithm::Munkres
cpanm Math::Combinatorics
```

</details>

<details>
<summary><b>âŒ Issue 3: Database Connection Failed</b></summary>

**Symptoms:** System runs in demo mode, no real data saved

**Solution:**
```bash
# Check MySQL is running
# Windows:
services.msc  # Look for MySQL service

# Linux:
sudo systemctl status mysql

# Test connection:
mysql -u harsh -p
# Enter password and verify connection

# Update credentials in main.py if needed
```

</details>

<details>
<summary><b>âŒ Issue 4: "Scorer script not found" Error</b></summary>

**Symptoms:** Evaluation fails with missing scorer.pl

**Solution:**

1. Ensure `scorer/` directory exists in project root
2. Download CorScorer from official source
3. Place `scorer.pl` and `CorScorer.pm` in `scorer/` directory
4. Verify files:
```bash
   ls scorer/
   # Should show: scorer.pl  CorScorer.pm
```

</details>

<details>
<summary><b>âŒ Issue 5: Port 8000 Already in Use</b></summary>

**Symptoms:** "Address already in use" error when starting

**Solution:**
```bash
# Find process using port 8000
# Windows:
netstat -ano | findstr :8000

# Linux/Mac:
lsof -i :8000

# Kill the process or use different port
# In main.py, change the last line:
uvicorn.run(app, host="0.0.0.0", port=8001)
```

</details>

<details>
<summary><b>âŒ Issue 6: File Upload Fails</b></summary>

**Symptoms:** "Only .txt files allowed" or upload errors

**Solution:**

1. âœ… Ensure file has `.txt` extension
2. âœ… Verify file format matches CoNLL/SemEval format
3. âœ… Check file size (very large files may timeout)
4. âœ… Ensure proper file permissions on `uploads/` directory

</details>

<details>
<summary><b>âŒ Issue 7: Scores Show as "N/A"</b></summary>

**Symptoms:** Evaluation completes but scores show N/A

**Solution:**

1. âœ… Check file format is correct
2. âœ… Verify gold dataset exists for selected language
3. âœ… Look at terminal output for Perl script errors
4. âœ… Ensure file encoding is UTF-8

</details>

---

### ğŸ› Debug Mode

Enable detailed logging by adding to `main.py`:
```python
import logging
logging.basicConfig(level=logging.DEBUG)
```

This will show detailed information about:
- ğŸ“Š Database queries
- ğŸ“ File operations
- ğŸ“œ Perl script execution
- âš ï¸ Error stack traces

---

## ğŸ¤ Contributing

Contributions are welcome! Here's how you can help:

### ğŸ Reporting Bugs

1. âœ… Check if the issue already exists
2. ğŸ“ Provide detailed description
3. ğŸ”„ Include steps to reproduce
4. ğŸ“‹ Share error messages and logs